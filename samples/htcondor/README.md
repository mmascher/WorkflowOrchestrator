# HTCondor Sample Submission

Sample HTCondor JDL and wrapper script for submitting StepChain jobs. Two submission modes are supported:

1. **One job per site** (`job.jdl`) — useful for validating StepChain execution across the Grid. Iterates over sites in `sitelist.txt`.
2. **DAG workflow** — one Condor job per event_splitter job. Run all split jobs via DAGMan, with retry-on-different-machine support.

## Directory Setup

Before submitting, create the required output directories:

```bash
mkdir -p log out err results
```

| Directory | Purpose |
|-----------|---------|
| `log/`    | HTCondor log files |
| `out/`    | Job stdout |
| `err/`    | Job stderr |
| `results/`| Transferred-back output tarballs |

## Input Files

Copy the following files into this directory:

1. **`execute_stepchain.sh`**, **`submit_env.sh`**, and **`stage_out.py`** from `ep_scripts/`:

   ```bash
   cp ../../ep_scripts/execute_stepchain.sh .
   cp ../../ep_scripts/submit_env.sh .
   cp ../../ep_scripts/stage_out.py .
   ```

2. **`WMCore.zip`** — Pre-packaged WMCore libraries for the worker. Must be present in this directory (provided in the repo).

3. **event_splitter output** — place it in an `event_splitter_out/` subdirectory:
   - **One-per-site mode:** `job0.json`, `job1.json`, …, `job<NSites-1>.json` (one per site)
   - **DAG mode:** `job1.json`, `job2.json`, … (1-based from event_splitter)
   - Both modes require `request_psets.tar.gz`

   See the [event_splitter README](../../src/python/job_splitters/README.md) for how to produce these files.

The final layout should look like:

```text
htcondor/
├── execute_stepchain.sh
├── submit_env.sh
├── stage_out.py
├── WMCore.zip
├── event_splitter_out/
│   ├── job0.json
│   ├── job1.json
│   ├── ...
│   └── request_psets.tar.gz
├── log/
├── out/
├── err/
├── results/
├── job.jdl
├── job.submit          # (DAG mode: generated by create_stepchain_dag.py)
├── stepchain.dag      # (DAG mode: generated by create_stepchain_dag.py)
├── run.sh
└── sitelist.txt
```

For **DAG mode**, `event_splitter_out/` must contain `job1.json`, `job2.json`, … (1-based from event_splitter).

## Submission

### One job per site

```bash
condor_submit job.jdl
```

This queues one job per site in `sitelist.txt`. Each job transfers `execute_stepchain.sh`, `submit_env.sh`, `stage_out.py`, `WMCore.zip`, the corresponding `job$(Process).json`, and `request_psets.tar.gz` to the worker node, runs the StepChain, and transfers the output tarball back into `results/`.

### DAG workflow (all event_splitter jobs)

To run **one Condor job per event_splitter job** (full event-based splitting):

1. **Run event_splitter** with `--output-dir` (e.g. `event_splitter_out/`):

   ```bash
   export PYTHONPATH=<path_to_WMCore>/src/python
   python ../src/python/job_splitters/event_splitter.py \
     --request <request.json> --splitting <splitting.json> \
     --psets <PSets/> --output-dir event_splitter_out/
   ```

2. **Create DAG and submit file** (proxy and sitelist required):

   ```bash
   python ../../src/python/job_splitters/create_stepchain_dag.py \
     --event-splitter-dir event_splitter_out/ \
     --proxy /tmp/x509up_u$(id -u) \
     --sitelist sitelist.txt
   ```

   This generates `stepchain.dag` and `job.submit`. The script requires `event_splitter_out/` with `job1.json`, `job2.json`, ..., and `request_psets.tar.gz`.

3. **Submit the DAG:**

   ```bash
   condor_submit_dag stepchain.dag
   ```

**Retry behavior:** If `run.sh` fails (e.g. CVMFS or site issues), the job is retried on a different machine (up to 3 times by default). A DAG-level RETRY adds another round if needed. Use `--max-retries` to change the job-level retry count.
